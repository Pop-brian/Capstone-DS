{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pop-brian/Capstone-DS/blob/main/SpaceX_Data_Splitting_and_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install seaborn if not already present in the environment\n",
        "# This command is necessary in many lab environments to ensure the module is installed.\n",
        "# Using '!' prefix allows running shell commands from a Python environment like a Jupyter cell.\n",
        "!pip install seaborn\n",
        "\n",
        "# Pandas is a software library written for the Python programming language for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
        "import numpy as np\n",
        "# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\n",
        "import matplotlib.pyplot as plt\n",
        "# Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\n",
        "import seaborn as sns\n",
        "# Preprocessing allows us to standardize our data\n",
        "from sklearn import preprocessing\n",
        "# Allows us to split our data into training and testing data\n",
        "from sklearn.model_selection import train_test_split # Imported for Task 3\n",
        "# Allows us to test parameters of classification algorithms and find the best one\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Logistic Regression classification algorithm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Support Vector Machine classification algorithm\n",
        "from sklearn.svm import SVC\n",
        "# Decision Tree classification algorithm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# K Nearest Neighbors classification algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# --- Data Loading URLs ---\n",
        "from js import fetch\n",
        "import io\n",
        "\n",
        "# URL for Features (X) - dataset_part_3.csv (one-hot encoded features)\n",
        "URL_X = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_3.csv'\n",
        "# URL for Target Variable Source (Y) - dataset_part_2.csv (contains the 'Class' column)\n",
        "URL_Y_Source = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_2.csv\"\n",
        "\n",
        "\n",
        "# Function to load data from URL\n",
        "async def load_data(url):\n",
        "    resp = await fetch(url)\n",
        "    text = io.BytesIO((await resp.arrayBuffer()).to_py())\n",
        "    return pd.read_csv(text)\n",
        "\n",
        "# Main asynchronous function to perform all tasks: data loading, preprocessing, splitting, and modeling\n",
        "async def run_data_preparation_and_modeling():\n",
        "    print(\"--- Starting Data Preparation and Modeling ---\")\n",
        "\n",
        "    # Load X (Features)\n",
        "    X_df = await load_data(URL_X)\n",
        "    X = X_df\n",
        "    print(\"X (Features) loaded successfully. Shape:\", X.shape)\n",
        "\n",
        "    # --- TASK 1: Create Y (Target Variable) ---\n",
        "    data = await load_data(URL_Y_Source)\n",
        "    Y = data['Class'].to_numpy()\n",
        "    print(\"\\n--- Task 1 Results (Y) ---\")\n",
        "    print(\"Y shape:\", Y.shape)\n",
        "\n",
        "\n",
        "    # --- TASK 2: Standardize the Feature Data X ---\n",
        "    transform = preprocessing.StandardScaler()\n",
        "    X = transform.fit_transform(X)\n",
        "    print(\"\\n--- Task 2 Results (Standardized X) ---\")\n",
        "    print(\"Standardized X shape:\", X.shape)\n",
        "\n",
        "\n",
        "    # --- TASK 3: Split the Data into Training and Test Sets ---\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X,\n",
        "        Y,\n",
        "        test_size=0.2,\n",
        "        random_state=2\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Task 3 Results (Train-Test Split) ---\")\n",
        "    print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}\")\n",
        "\n",
        "\n",
        "    # Initialize dictionary to store accuracies for Task 12\n",
        "    model_accuracies = {}\n",
        "\n",
        "\n",
        "    # --- TASK 4: Logistic Regression Model Training and Hyperparameter Tuning ---\n",
        "    print(\"\\n--- TASK 4: Logistic Regression (GridSearchCV) ---\")\n",
        "\n",
        "    # 1. Define parameters and model object\n",
        "    parameters_lr ={'C':[0.01,0.1,1],\n",
        "                 'penalty':['l2'],\n",
        "                 'solver':['lbfgs']}\n",
        "\n",
        "    # Define model and ensure it's available\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    lr=LogisticRegression()\n",
        "\n",
        "    # 2. Create GridSearchCV object and ensure it's available\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "    logreg_cv = GridSearchCV(\n",
        "        estimator=lr,\n",
        "        param_grid=parameters_lr, # Using parameters_lr\n",
        "        scoring='accuracy',\n",
        "        cv=10,\n",
        "        verbose=0, # Set verbose to 0 to minimize output during fitting\n",
        "        n_jobs=-1 # Use all available cores\n",
        "    )\n",
        "\n",
        "    # 3. Fit the object to the training data\n",
        "    logreg_cv.fit(X_train, Y_train)\n",
        "\n",
        "    # 4. Output the results\n",
        "    print(\"\\nLogReg GridSearchCV completed.\")\n",
        "    print(\"Tuned hpyerparameters (best parameters): \", logreg_cv.best_params_)\n",
        "    print(\"Accuracy on validation data (best score): \", logreg_cv.best_score_)\n",
        "\n",
        "\n",
        "    # --- TASK 5: Calculate Accuracy on Test Data (Logistic Regression) ---\n",
        "    print(\"\\n--- TASK 5: Test Data Accuracy (Logistic Regression) ---\")\n",
        "\n",
        "    # Calculate the accuracy on the test data using the score method\n",
        "    lr_test_accuracy = logreg_cv.score(X_test, Y_test)\n",
        "    print(f\"Accuracy on test data (Logistic Regression): {lr_test_accuracy}\")\n",
        "    model_accuracies['Logistic Regression'] = lr_test_accuracy\n",
        "\n",
        "    # --- TASK 6: Support Vector Machine (SVM) Model Training and Hyperparameter Tuning ---\n",
        "    print(\"\\n--- TASK 6: Support Vector Machine (GridSearchCV) ---\")\n",
        "\n",
        "    # 1. Define parameters and model object\n",
        "    parameters_svm = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'),\n",
        "                      'C': np.logspace(-3, 3, 5),\n",
        "                      'gamma': np.logspace(-3, 3, 5)}\n",
        "\n",
        "    # Define SVM model and ensure it's available\n",
        "    from sklearn.svm import SVC\n",
        "    svm = SVC()\n",
        "\n",
        "    # 2. Create GridSearchCV object\n",
        "    svm_cv = GridSearchCV(\n",
        "        estimator=svm,\n",
        "        param_grid=parameters_svm,\n",
        "        scoring='accuracy',\n",
        "        cv=10,\n",
        "        verbose=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # 3. Fit the object to the training data\n",
        "    svm_cv.fit(X_train, Y_train)\n",
        "\n",
        "    # 4. Output the results\n",
        "    print(\"\\nSVM GridSearchCV completed.\")\n",
        "    print(\"Tuned hpyerparameters (best parameters): \", svm_cv.best_params_)\n",
        "    print(\"Accuracy on validation data (best score): \", svm_cv.best_score_)\n",
        "\n",
        "    # --- TASK 7: Calculate Accuracy on Test Data (SVM) ---\n",
        "    print(\"\\n--- TASK 7: Test Data Accuracy (SVM) ---\")\n",
        "\n",
        "    # Calculate the accuracy on the test data using the score method\n",
        "    svm_test_accuracy = svm_cv.score(X_test, Y_test)\n",
        "    print(f\"Accuracy on test data (SVM): {svm_test_accuracy}\")\n",
        "    model_accuracies['SVM'] = svm_test_accuracy\n",
        "\n",
        "    # --- TASK 8: Decision Tree Classifier Model Training and Hyperparameter Tuning ---\n",
        "    print(\"\\n--- TASK 8: Decision Tree Classifier (GridSearchCV) ---\")\n",
        "\n",
        "    # 1. Define parameters and model object\n",
        "    parameters_tree = {'criterion': ['gini', 'entropy'],\n",
        "                       'splitter': ['best', 'random'],\n",
        "                       'max_depth': [2*n for n in range(1, 10)],\n",
        "                       'max_features': ['auto', 'sqrt'],\n",
        "                       'min_samples_leaf': [1, 2, 4],\n",
        "                       'min_samples_split': [2, 5, 10]}\n",
        "\n",
        "    # Define Decision Tree model and ensure it's available\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    tree = DecisionTreeClassifier()\n",
        "\n",
        "    # 2. Create GridSearchCV object\n",
        "    tree_cv = GridSearchCV(\n",
        "        estimator=tree,\n",
        "        param_grid=parameters_tree,\n",
        "        scoring='accuracy',\n",
        "        cv=10,\n",
        "        verbose=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # 3. Fit the object to the training data\n",
        "    # Note: Fitting a complex GridSearch object like this can be computationally intensive and take time.\n",
        "    tree_cv.fit(X_train, Y_train)\n",
        "\n",
        "    # 4. Output the results\n",
        "    print(\"\\nDecision Tree GridSearchCV completed.\")\n",
        "    print(\"Tuned hpyerparameters (best parameters): \", tree_cv.best_params_)\n",
        "    print(\"Accuracy on validation data (best score): \", tree_cv.best_score_)\n",
        "\n",
        "\n",
        "    # --- TASK 9: Calculate Accuracy on Test Data (Decision Tree) ---\n",
        "    print(\"\\n--- TASK 9: Test Data Accuracy (Decision Tree) ---\")\n",
        "\n",
        "    # Calculate the accuracy on the test data using the score method\n",
        "    tree_test_accuracy = tree_cv.score(X_test, Y_test)\n",
        "    print(f\"Accuracy on test data (Decision Tree): {tree_test_accuracy}\")\n",
        "    model_accuracies['Decision Tree'] = tree_test_accuracy\n",
        "\n",
        "\n",
        "    # --- TASK 10: K Nearest Neighbors (KNN) Model Training and Hyperparameter Tuning ---\n",
        "    print(\"\\n--- TASK 10: K Nearest Neighbors (GridSearchCV) ---\")\n",
        "\n",
        "    # 1. Define parameters and model object\n",
        "    parameters_knn = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "                      'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "                      'p': [1,2]}\n",
        "\n",
        "    # Define KNN model and ensure it's available\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    knn = KNeighborsClassifier()\n",
        "\n",
        "    # 2. Create GridSearchCV object\n",
        "    knn_cv = GridSearchCV(\n",
        "        estimator=knn,\n",
        "        param_grid=parameters_knn,\n",
        "        scoring='accuracy',\n",
        "        cv=10,\n",
        "        verbose=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # 3. Fit the object to the training data\n",
        "    knn_cv.fit(X_train, Y_train)\n",
        "\n",
        "    # 4. Output the results\n",
        "    print(\"\\nKNN GridSearchCV completed.\")\n",
        "    print(\"Tuned hpyerparameters (best parameters): \", knn_cv.best_params_)\n",
        "    print(\"Accuracy on validation data (best score): \", knn_cv.best_score_)\n",
        "\n",
        "\n",
        "    # --- TASK 11: Calculate Accuracy on Test Data (KNN) ---\n",
        "    print(\"\\n--- TASK 11: Test Data Accuracy (KNN) ---\")\n",
        "\n",
        "    # Calculate the accuracy on the test data using the score method\n",
        "    knn_test_accuracy = knn_cv.score(X_test, Y_test)\n",
        "    print(f\"Accuracy on test data (KNN): {knn_test_accuracy}\")\n",
        "    model_accuracies['K-Nearest Neighbors'] = knn_test_accuracy\n",
        "\n",
        "\n",
        "    # --- TASK 12: Find the Best Performing Model ---\n",
        "    print(\"\\n--- TASK 12: Model Performance Summary ---\")\n",
        "\n",
        "    # Find the best performing model\n",
        "    best_model_name = max(model_accuracies, key=model_accuracies.get)\n",
        "    best_accuracy = model_accuracies[best_model_name]\n",
        "\n",
        "    # Create a DataFrame for easy comparison\n",
        "    accuracy_df = pd.DataFrame(\n",
        "        list(model_accuracies.items()),\n",
        "        columns=['Model', 'Test Accuracy']\n",
        "    ).sort_values(by='Test Accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Print the comparison table\n",
        "    print(\"\\nComparison of Model Test Accuracies:\")\n",
        "    print(accuracy_df.to_markdown(index=False))\n",
        "\n",
        "    print(f\"\\nConclusion: The best performing model on the test data is '{best_model_name}' with an accuracy of {best_accuracy:.4f}.\")\n",
        "\n",
        "\n",
        "    # Return all necessary objects: the trained GridSearchCV objects and the split data\n",
        "    return logreg_cv, svm_cv, tree_cv, knn_cv, X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# Execute the main data preparation and modeling function\n",
        "logreg_cv_result, svm_cv_result, tree_cv_result, knn_cv_result, X_train, X_test, Y_train, Y_test = await run_data_preparation_and_modeling()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "KdY2R_8OxJma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bb"
      ],
      "metadata": {
        "id": "F5RCpKVYfDbg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}